Written on October 17

# 파이프라인

각각 단계별 서로 다른 태스크로 구성

각 태스크들은 정해진 순서대로 진행되어야만 함

즉, 태스크들 간의 의존성이 존재하게 됨

태스크 간의 의존성을 명확하게 확인하는 방법 중 하나는 그래프로 표현하는 것

-   태스크 -> 노드
-   의존성 -> 노드 간 화살표

이러한 그래프는 방향성을 가지기 때문에 방향성 그래프(directed graph)라고 함

또한 태스크는 한번 실행 후 다시 실행되면 안된다. 그렇기에 최종적으로 이러한 그래프를 방향성 비순환 그래프(Directed Acyclic Graph, DAG)라고 부른다.

반복, 순환을 허용하지 않음 (순환시 의존성 문제가 생길 수 있음)

논리적 오류는 교착상태(deadlock)을 일으키며 그래프가 동작하지 않게 된다.



또한 그래프 실행 알고리즘을 적용하면 태스크를 병렬로 실행할 수 있기 때문에 가용 컴퓨팅 리소스를 더 효율적으로 활용할 수 있음
-> 태스크 실행 시간을 감소시킬 수 있음



모놀리식 스크립트로도 각 태스크를 진행 할 수 있지만 태스크 실패에 대한 대응이 어렵다. (일부에서 실패시 전체 스크립트를 재실행 해줘야만 함)

그래프 기반에선 실패한 태스크만 재실행하면 되므로 효율적



# Airflow

파이썬을 사용하여 워크플로으 태스크를 DAG로 정의할 수 있음

DAG파일에 파이썬 코드로 DAG를 정의함



DAG 파일은 주어진 DAG에 대한 태스크 집합과 태스크 간의 의존성 기술함

Airflow는 DAG 구조를 식별하기 위해 코드를 파싱(parsing)함



3rd party 모듈로 airflow 확장 가능함(확장성)

파이썬 프로그래밍으로 유연성을 갖음(유연성)

여러 시스템 간에 데이터 프로세스를 결합할 수 있는 복잡한 데이터 파이프라인 구축을 가능하게 함



DAG로 구조 정의 후 DAG의 실행 주기를 정의할 수 있음

airflow에 DAG 실행 설정을 하고 cron과 같은 표현식으로 좀 더 복잡한 스케줄링이 가능함





airflow는 크게 3가지 구성요소를 갖음

-   Airflow Scheduler
    -   DAG를 분석하고 현재 시점에서 DAG의 스케줄이 지난 경우 Airflow 워커에 DAG의 태스크를 예약
-   Airflw worker
    -   예약된 태스크를 선택하고 실행
-   Airflow Web server
    -   스케줄러에서 분석한 DAG를 시각화하고 DAG 실행과 결과를 확인 할 수 있는 주요 인터페이스 제공



## 스케줄러

실행 단계

1.   스케줄러가 DAG 파일을 분석하고 각 DAG 태스크, 의존성 및 예약 주기 확인

2.   처음부터 끝까지 DAG 내용 확인 후 DAG의 예약 주기가 경과 했는지 확인

3.   예약 주기가 현재 시간 이전(아직 안지났으면)이면 실행되도록 예약

4.   해당 태스크의 의존성(=업스트림 태스크)을 확인 후, 의존성 태스크가 완료되지 않았으면 실행 대기열에 추가
5.   스케줄러는 1번으로 가서 다음 루프를 대기함









## DAG 작성

-   dag 정의 (DAG생성)
-   오퍼레이터 작성
-   태스크 실행 순서 설정(의존성)

```python
import json
import pathlib

import airflow.utils.dates
import requests
import requests.exceptions as requests_exceptions
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

dag = DAG(
    dag_id="download_rocket_launches",
    description="Download rocket pictures of recently launched rockets.",
    start_date=airflow.utils.dates.days_ago(14),
    schedule_interval="@daily",
)

download_launches = BashOperator(
    task_id="download_launches",
    bash_command="curl -o /tmp/launches.json -L 'https://ll.thespacedevs.com/2.0.0/launch/upcoming'",  # noqa: E501
    dag=dag,
)


def _get_pictures():
    # Ensure directory exists
    pathlib.Path("/tmp/images").mkdir(parents=True, exist_ok=True)

    # Download all pictures in launches.json
    with open("/tmp/launches.json") as f:
        launches = json.load(f)
        image_urls = [launch["image"] for launch in launches["results"]]
        for image_url in image_urls:
            try:
                response = requests.get(image_url)
                image_filename = image_url.split("/")[-1]
                target_file = f"/tmp/images/{image_filename}"
                with open(target_file, "wb") as f:
                    f.write(response.content)
                print(f"Downloaded {image_url} to {target_file}")
            except requests_exceptions.MissingSchema:
                print(f"{image_url} appears to be an invalid URL.")
            except requests_exceptions.ConnectionError:
                print(f"Could not connect to {image_url}.")


get_pictures = PythonOperator(
    task_id="get_pictures", python_callable=_get_pictures, dag=dag
)

notify = BashOperator(
    task_id="notify",
    bash_command='echo "There are now $(ls /tmp/images/ | wc -l) images."',
    dag=dag,
)

download_launches >> get_pictures >> notify
```

여기선 BashOperator와 PythonOperator 2개를 사용



-   dag docs
    -   https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/dag/index.html
-   python api의 기본 operator
    -   https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.html



각 오퍼레이터는 하나의 태스크를 수행하고 여러 개의 오퍼레이터가 airflow의 워크플로우 또는 DAG를 구성한다.

오퍼레이터 끼리 서로 독립적으로도 실행 가능, or 순서 정의해 순서대로 실행도 가능
--> 이를 의존성(dependency)이라고 정의함

오른쪽 시프트 연산자(binary right shift operator), rshift (>>)로 태스크간 의존성 정의





task vs. operator

둘다 비슷. 일단 코드를 실행함

operator : single piece of resplonsibility

단일 책임, 단일 작업 수행



DAG의 역할은 오퍼레이터 컬렉션 실행의 오케스트레이션 역할

오퍼레이터의 시작과 끝, 연속 작업, 의존성 보장

Airflow는 BaseOperator와 BaseOperator로부터 상속된 PythonOperator, EmailOeprator, OracleOperator와 같은 다양한 서브 클래스 제공



task는 operator가 정상적으로 올바른 실행을 보장하기 위한 오퍼레이터의 매니저 (or wrapper)라고 보면 됨

airflow는 task를 통해 작업을 올바르게 실행할 수 있음



### basic

DAG

-   from airflow import DAG
-   DAG(dag_id, start_date, schedule_interval)



BashOperator

-   from airflow.operators.bash import BashOperator
-   BashOperator(task_id, bash_command, dag=dag)

PythonOperator

-   from airflow.operators.python import PythonOperator
-   PythonOperator(task_id, python_callable, dag=dag)
    -   python_callable은 실행할 파이썬 함수를 지칭



모든 오퍼레이터는 task_id가 필요함
태스트 실행 시 참조되며, Airflow UI에 표시됨



## DAG 실행

Airflow 구성

-   스케줄러
-   웹 서버
-   데이터베이스

dag파일이 DAG 스크립트가 저장될 위치 디렉토리(지정한 곳)에 있으면 자동으로 Airflow가 그 스크립트를 읽고 DAG를 구성하는 비트와 조각들을 꺼내에 웹UI에 시각화 해준다.



airflow ui에서 DAG on/off **토글**에서 on으로 하여 DAG를 올려주고, trigger를 run해줘서 DAG를 실행할 수 있다.



### 스케줄링 기본

DAG를 일정 시간 간격으로 실행할 수 있도록 스케줄을 설정할 수 있다.

DAG 인스턴스에 `schedule_interval=sth`를 설정해주면 된다. 

인자로 올 수 있는 값

-   cron string
-   timedelta object (datetime에 있는 듯)
-   Timetalbe
-   list of Dataset

커스텀 스케줄 https://airflow.apache.org/docs/apache-airflow/stable/howto/timetable.html

DAG Runs https://airflow.apache.org/docs/apache-airflow/stable/dag-run.html?highlight=daily



예시) schedule_interval="@daily"

매일 하루 한번씩 트리거된다.

@daily는 `0 0 * * *`이랑 같음. airflow가 @daily를 `* * 0 0 0`으로 변경해 읽음

사용 가능한 Cron preset (@daily같은걸 cron preset이라 하나봄)

| preset     | meaning                                     | cron        |
| ---------- | ------------------------------------------- | ----------- |
| None       | 스케줄X, 외부 트리거<br />tigger눌러야 실행 |             |
| @once      | 한번만 예약                                 |             |
| @hourly    | 한시간에 한번씩                             | 0 * * * *   |
| @daily     | 하루(밤12시 정각)에 한번씩                  | 0 0 * * *   |
| @weekly    | 매주 일요일마다 한번씩(24:00)               | 0 0 * * 0   |
| @monthly   | 그달의 첫날 한번씩(24:00)                   | 0 0 1 * *   |
| @quarterly | 1/4분기 달 첫날에 한번씩(24:00)             | 0 0 1 */3 * |
| @yearly    | 매년 1월1일에 한번씩(24:00)                 | * * 1 1 *   |





### 실패시엔?

태스크는 여러 가지 알 수 없는 이유, 대처할 수 없는 이유 등으로 실패할 수 있음

-   외부 서비스 중단
-   네트워크 연결 문제
-   디스크 손상
-   등등

실패한 태스크는 그래프 뷰, 트리 뷰에서 **빨간색**으로 표시된다.

또한 의존성에 묶여 앞에 실패가 뜬 task가 있으면 뒤에 있는 task는 실행되지 않기에 **주황색**으로 표시된다.

기본적으로 이전 태스크가 모두 성공해야 하고, 실패하면 뒤에 태스크는 실행되지 않는다.



airflow는 실패시 전체 작업을 다시 실행할 필요 없이 실패한 task를 클릭 후 `Clear` 버튼을 누른다. 초기화된 태스크가 표시되고(reset상태), Airflow는 이 태스크를 재실행 한다.









## 스케줄링

챕터3장에서 배울 것들

-   스케줄 간격으로 증분 데이터 처리 방법??
-   웹 사이트의 사용자 이벤트를 분석하는 간단한 사례 / DAG를 구성해 정기적으로 사용자 이벤트를 분석
-   데이터 분석에 대해 점진적인 접근 방법 확인하고 Airflow의 실행 날짜 개념과 어떻게 연관되는지를 통해 이 과정을 더욱 효율적으로 만드는 방법을 살펴보자.
-   백필(backfilling)을 사용해 데이터 세트의 과거 공백을 어떻게 채울까?
-   Airflow 태스크의 몇 가지 중요한 속성에 대해





예시: 사용자 이벤트 처리

웹 페이지 사용자 동작 추적, 액세스한 페이지 분석(IP주소로 식별)하는 서비스

--> 1차. 어떤 페이지 접근, 얼마나 시간을 쓰는지 알고싶다

--> 2차. 시간이 지남에 따라 사용자 행동이 어떻게 변하는지 알고 싶다. (통계 계산을 하려고)

>   일반적으로 외부 추적 서비스는 실용성 이유로 30일 이상 데이터를 저장해두지 않는다 함

raw 데이터는 매우 클 수 있기에 S3, Cloud Storage같은 데이터레이크에 저장함
(높은 내구성, 상대적으로 싼 비용)





### airflow의 스케줄 기준

```python
dag=DAG(
	dag_id="daily_schedule",
	schedule_interval="@daily",
	start_date=dt.datetime(2019, 1, 1),
)
```

DAG를 정의할 때

-   schedule_interval : 실행 주기
-   start_date : 시작 날짜

Airflow는 시작 날짜를 기준으로 첫 번째 DAG의 실행을 스케줄(시작날짜+간격)한다.

이게 무슨말인가

정의는 2019년 1월1일 시작이고 매일 실행(@daily)이다. 만약 1월1일 오후쯤 실행을 하면 자정이 오기전까지 DAG는 어떤 작업도 하지 않음

종료일을 정해주지 않으면 DAG는 실행 후 interval에 맞게끔 계속 돌아간다.(이론적으론 영원히...)
end_date인수를 줘서 끝낼 수 있음



```python
dag=DAG(
	dag_id="daily_schedule",
	schedule_interval="@daily",
	start_date=dt.datetime(2019, 1, 1),
    end_date=dt.datetime(2019, 1, 5),
)
```

1월 1일 시작, 1월5일 끝

--> 1/2 첫 시작, ..., 1/6일 실행하고 끝

### preset

schedule_interval을 정의할 때 preset을 쓸 수 있음

### cron

cron도 쓸 수 있음

### timedelta

cron의 제약은 특정 빈도(frequency)마다 스케줄을 정의할 수 없음

3일마다 작업을 실행하게 하고 싶은데 1, 4, 7, ..., 28, 31..-> 1일..?

이러한 문제점을 해결하는게 빈도 기반 스케줄인 timedelta 인스턴스를 쓰면됨 (datetime에 포함된)

```python
import datetime as dt

dag=DAG(
	dag_id="timedelta",
	schedule_interval=dt.timedelta(days=3),
	start_date=dt.datetime(year=2019, month=1, day=1),
	end_date=dt.datetime(year=2019, month=1, day=5),
)
```

dt.timedelta에 days=3이라고 하면 빈도로 3일마다 돌아가게 된다.

days, minutes, hours등등 설정도 됨



### 데이터 증분 처리

매일 매일 데이터를 가져와서 처리하는 워크플로우를 만들었다. 근데 매일매일 특정 데이터를 모두 가져와서 처리하는건 비효율적, 현실적으로 불가...

비유를 하자면 데이터베이스 테이블 전체를 다 SELECT해서 처리하겠다는 건데...

데이터 증분 처리를 한다는건 스케줄 간격에 해당하는 데이터만 가져와서 처리한다는 것

#### 이벤트 데이터 증분

데이터를 순차적으로 가져올 수 있도록 DAG를 변경

스케줄 간격에 해당하는 일자의 이벤트만 로드하고 새로운 이벤트만 통계 계산

--> 증분 방식(incremental approach)는 스케줄된 하나의 작업에서 처리해야 할 데이터 양을 크게 줄일 수 있기 때문에, 전체 데이터 세트를 처리하는 것보다 훨씬 효율적이다.



어떻게?

BashOperator에서 curl로 데이터를 가져온다 하면 쿼리 파라미터로 시작일, 종료일을 지정해두면 기간으로 가져올 수 있기는 함

근데 매번 이 파라미터를 바꿔주기 힘들다.

시간 기반 프로세스(time-based process) 워크플로우의 경우, 주어진 작업이 실행되는 시간 간격을 아는 것이 중요함

--> Airflow는 몇가지 추가 매개변수 제공

추가 매개변수 중 가장 중요한 매개변수는 DAG가 실행되는 날짜와 시간을 나타내는 `execution_date`임

DAG를 시작하는 시간의 특정 날짜가 아니라 스케줄 간격으로 실행되는 시작 시간을 나타내는 타임스탬프이다.
스케줄 간격의 종료 시간은 next_execution_date라는 매개변수 사용
이를 통해 태스크의 스케줄 간격을 정의함

 

