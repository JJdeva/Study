# 데이터 엔지니어링

data engineering

데이터 엔지니어링 분야에서 흔히 쓰는 도구들과 기법, 분야 소개

도구들을 조합해 데이터 파이프라인(data pipeline)을 구축하는 방법



> 데이터 공학은 데이터 기반구조를 개발하고 운영하고 유지보수하는 활동이다. 여기서 데이터 기반 구조는 데이터를 추출, 변환, 적재하기 위한 파이프라인과 데이터베이스로 구성되며, 조직이 직업 운영할 수도 있고(on-premise) 클라우드에 둘 수도 있다. (또는 둘의 결합 형태)





다수의 데이터 공급원(data source)와 연결

-> extract

-> transform

-> loading

(ETL)





## ETL

가장 낮은 수준에서 데이터 공학에는 데이터를 한 시스템에서 다른 시스템으로 이동하거나 다른 형식(format)으로 변환하는 작업이 관여한다.

데이터 공급원에서 데이터를 질의하고(추출), 데이터를 어떤 방식으로 수정하고(변환), 데이터를 사용자가 접근할 수 있는, 그리고 거기에 있는 데이터가 실무 품질임을 아는 어떤 장소에 넣는다(적재)

추출(extract), 변환(transform), 적재(load)라는 ETL 프로세스



데이터의 규모가 작을 때는 굳이 데이터 엔지니어가 필요하지 않음.

근데 프로덕션환경을 넘어 빅데이터로 가면 데이터 엔지니어가 필요해짐

데이터의 양이 많아지고, 데이터베이스가 하나가 아닌 여러개고, 위치도 서로 다를 수 있음

즉, 문제의 규모가 커지면 데이터 엔지니어링이 필요해짐



데이터 엔지니어는 모든 지역(region)의 데이터베이스에 연결해서 데이터를 추출하고 데이터 웨어하우스에 적재한 후 그곳에서 분석을 진행한다.

단순히 추출해 적재하는 것으로는 데이터를 활용한 업무를 하기가 힘들다. 그렇기에 추출과 적재 사이에 변환하는 작업이 필요

또한 시간대, 지역도 고려해야할 사항 (timezone, location)



> 시간 데이터 - 국제 표준화 기구(ISO)가 제정한 ISO 8601 표준이 있음



예시)

- 각 데이터베이스에서 데이터를 추출한다.
- 데이터의 각 거래에 해당 위치를 가리키는 필드를 추가한다.
- 날짜를 지역 시간 형식에서 ISO 8601 형식으로 변환한다.
- 데이터를 데이터 웨어하우스에 적재한다.



**<u>그림추가할것</u>**



데이터를 추출, 변환, 적재를 위해서는 데이터 파이프라인이 필요함

데이터는 파이프라인에 미가공된(raw) 형태로 들어옴

결측값, 오타 등이 있는 dirty한 상태로 들어옴



## 지식

DE에 필요한 지식과 기술

- 추출
    - 다양한 형식의 파일이나 다양한 종류의 데이터베이스에서 데이터를 추출하는 방법을 알아야 한다.
        - 즉, SQL, 파이썬 / 다양한 데이터 포맷에 대한 지식과 변환방법

- 변환
    - 데이터 모델링(modeling)과 구조에 익숙해야함
    - 업무(비즈니스)를 이해하고 데이터를 뽑아내고자 하는 지식과 통찰(insight)
- 데이터 웨어하우스
    - 데이터를 적재할 스키마를 갖춘 데이터 웨어하우스를 구축하고 관리하는 능력
    - 데이터 웨어하우스 설계의 기초
    - 데이터 웨어하우스에 쓰이는 데이터베이스 종류와 사용법
- 인프라 설계 / 인프라 관리
    - 리눅스 서버 관리 능력
    - airflow, nifi같은 파이프라인 관리 툴 (오케스트레이션?)
    - 클라우드에 데이터 인프라 구축 능력 (AWS, GCP, AZURE)



## 조건

데이터 엔지니어링 tool는 다음 조건을 만족해야함

빅데이터의 3v

- volume
    - 데이터의 크기가 커짐
    - 빠른 시간안에 레코드 수백만개를 옮기거나 수백만 건의 트랜잭션 처리가 가능해야할 정도
- variety
    - 다양한 소스에 있는 다양한 데이터를 처리하는 도구가 필요함
- velocity
    - 실시간성(real time, streaming data processing)으로 작동하는 도구들도 필요할 때가 있음



## 기술

### SQL

데이터 왕국의 만국 공용어는 SQL이다.

어느 데이터 분야의 시민이 되어도 SQL은 필수

SQL 최적화를 통해 쿼리 속도를 높일 수 있고, 데이터 변환 작업에도 도움이 된다.

데이터 레이크나 NoSQL에서도 SQL을 지원하기도 한다.



- 자바와 친구들
    - 오픈 소스 진영의 데이터 툴들은 자바와 스칼라를 많이 사용한다.
        (자바는 점점 같은 JVM에서 돌아가는 다른 언어들로 대체되고 있다함...)
    - JVM에서 도는 언어는
        - Scala
        - Clojure
        - Groovy
- 파이썬
    - 데이터 과학분야에서는 파이썬을 따라갈 언어가 없다.
    - numpy, pandas, scikit-learn, tensorflow, pytorch 등 좋은 라이브러리가 많음



- Apache NiFi
    - 자바, Clojure, Groovy, Jython을 이용해 커스텀 데이터 처리기(data processor)를 개발할 수 있음





### DB

데이터 베이스

대부분 실무 시스템에서는 데이터는 관계형 데이터베이스(relational database)에 저장됨
보통 이런 프로덕션 OLTP RDB들은 로우 베이스로 데이터를 저장하는데, 효율적인 트랜잭션에 적합한 형태다.

데이터베이스, 테이블 간에 관계가 존재함. 키를 기반으로 조인함



### DW

데이터 웨어하우스

대표적인 dw

- AWS Redshift
- Google BigQuery
- Snowflake
- Apache Cassandra

NoSQL

- elk stack 친구들
- DynamoDB
- MongoDB (DocumentDB) 등등

보통 데이터웨어하우스는 컬럼베이스 데이터베이스다. 데이터를 컬럼을 기준으로 저장해서 쿼리 속도가 빠르기 때문에 큰 데이터를 다루는 데이터 웨어하우스에 적합하다.

(카산드라는 자체적은 카산드라 쿼리 언어를 씀)





### Data processing

데이터를 처리할 때는 데이터 처리 엔진(data processing engine)을 이용해서 데이터를 일괄(batch) 또는 연속적(stream)으로 변환함
(변환 작업은 병렬적으로도 처리 가능)



가장 대표적인 엔진은 **아파치 스파크(Apache Spark)**다.

파이썬, 자바, 스칼라 등의 언어를 사용해서 다룰 수 있고 다양한 데이터 형식도 지원한다.

- RDD (Resilient Distributed Dataset) - 복원성 데이터 집합
    - 객체들의 불변적인(immutable) 분산 컬렉션(distributed collection)
    - 데이터 처리를 빠르고 분산된 방식으로 수행가능하게 함
    - 하나의 RDD에 속한 여러 작업은 클러스터 안의 여러 노드에서 실행됨. DataFrame과는 달리 RDD는 데이터의 스키마를 추측하려 하지 않음
- DataFrame
- SparkSQL

pandas에 익숙한 사람은 Spark DataFrame에 친숙할 수도 있음





아파치 스톰(Apache Storm)

- Spout (스파우트)라는 처리 단위를 이용해 데이터를 읽고 처리함

아파치 플링크(Apache Flink) / 삼자(Samza)

- 좀 더 현대적인 스트림 및 배치 처리 프레임워크로, 무제한 스트림의 처리를 지원함
- 무제한 스트림(unbounded stream)이란 데이터의 끝이 정해져 있지 않은 스트림
    - 예를 들어 온도 측정기에서 공급되는 온도 수치들이 무제한 스트림이다. (고장나지 않는 이상 온도 데이터는 계속 들어온다.)
- 카프카를 이용해 데이터를 스트리밍한다면 플링크와 삼자와의 조합이 좋다.





## Data pipeline

- 트랜잭션 데이터베이스
- 프로그래밍 언어
- 처리 엔진
- 데이터 웨어하우스



이들을 조합하면 하나의 파이프라인이 만들어 진다.

데이터베이스에서 레코드를 모두 끌고와서 프로그래밍 언어와 처리엔진으로 변환하고 데이터웨어하우스에 적재하는게 하나의 파이프라인이다.

근데 이러한 파이프라인을 구동할때마다 사람이 수동으로 하는건 효율적이지 못함



그렇기에 데이터 파이프라인에는 스케줄러가 있어야 한다.

가장 간단한건 cron을 이용해 스크립트를 주기마다 실행해주는 것

근데 파이프라인이 많아지고 복잡해지면 cron으로만으로는 관리가 안된다.
또한 작업 실패와 의존성문제, 배압(backpressure)문제는 어떻게 대처하고 처리해야하나?

> 배압이 역으로 순환하려는걸 의미하는 건가? (Acyclic graph가 아닌 cycle이 문제가 되는게 backpressure인가? 이건좀 찾아보자)

이런 복잡한 문제는 cron으론 해결이 안된다.



### airflow

파이썬을 사용한 데이터 파이프라인에서 가장 인기있는게 Apache Airflow다.

- Airbnb가 개발한 workflow(워크플로) 툴
- 서버, 스케줄러, 메타데이터스토어, 대기열 처리(queing) 시스템, 다수의 실행기(excutor)들로 구성됨
- 하나의 인스턴스로도 실행 할 수 있고, 여러 실행기 노드들로 분할하고 그 노드들로 하나의 클러스터를 형성해서 실행할 수도 있다.
    실무에선 멀티노드로 많이씀
- DAG를 지원한다.
    - DAG를 이루는 다양한 tasks들로 정의됨
    - 방향성이 있어 의존성문제를 해결할 수 있다.
- 웹UI를 지원해서 웹에서 작업을 확인하고 트리거할 수 있다.



### NiFi

- 미국 국가안보국(national Security Agency)이 만듦
- airflow와 같이 DAG를 사용한다.
- 설정이 쉬워 러닝커브가 비교적 낮다
- GUI가 airflow보다 좋음
- python, Jython, Clojure, Scala, Groovy 언어를 써서 사용할 수 있음
- 파이프라인 클러스터링과 원격 실행 지원
- 스케줄러 내장, 파이프라인의 배압 처리 및 모니터링 기능도 제공
- NiFi 레지스트리를 이용한 버전 관리 기능
- edge(시스템의 최외곽 경계)에서 데이터를 수집하기 위한 MiFiNi도 제공



그외 루이지도 있음





---

가장 많이 쓰이는 언어

- 파이썬 / 스칼라 / 자바

트랜잭션 데이터베이스 / 데이터 웨어하우스

- 온프레미스 / 클라우드 구축



프로그래밍, 운영, 데이터 모델링, 데이터베이스, 운영체제 등 다양한 분야 지식이 필요함
